# -*- coding: utf-8 -*-
"""Customer_Segment_Odo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gXmrooiKnDsmoaD7qTPQFVU_Y_rV_JMK

# Create the User Journey based on the clickstream

### For user journey data I have to analyse which features will contribute to this purpose, here I am using the following features

### 1)To identify unique users: 'userId'
### 2)To track the version of the application 'app_release' or 'ios_app_release' or 'android_app_version' or 'ios_app_version'
### 3)To determine the timestamp of each event 'time' or '$mp_api_timestamp_ms'
### 4)To identify the events or scenes'currentScene'
### 5)To filter events based on the desired time period 'Month', 'day', 'Year'

##Following things I am doing for the Efficient Memory Usage

1. **Chunk Processing**: Instead of loading the entire dataset into memory, I have used chunk processing when reading the CSV file to reduce memory usage.

2. **Dropping Unnecessary Features**: by dropping the unnecessary features, I am avoiding the need to load unnecessary data into memory.

3. **Data Type assign**: By assigning data types of each features, it will help in fast loading of datasets, which reduce memory.
"""


# Things to keep in mind that this code will store two output files in your device at a given location whose directories are given as '/content/new_csv_files' and '/content/combined_csv_files', because this is for not harming the original csv file

import pandas as pd
from datetime import datetime,timedelta
import csv
from collections import Counter,defaultdict
import os



class DataAnalyzer:
    def __init__(self, data_directory):
        self.data_directory = data_directory
        self.data = None

    def process_data(self, columns_to_check):
        try:
            # Step 1: Check columns in CSV files
            csv_files = self.get_csv_files()
            missing, found = self.check_columns_in_csv_files(csv_files, columns_to_check)

            # Step 2: Create a new directory to save modified CSV files
            new_directory = self.create_new_directory()

            # Step 3: Modify CSV files, drop null values, and save them to the new directory using chunking
            for csv_file in found:
                self.modify_csv_file(csv_file, new_directory)

            # Step 4: Convert distinct_id to numerical values
            folder_path = new_directory
            self.convert_distinct_id_to_numerical(folder_path)

            # Step 5: Concatenate all CSV files and save the result to a new directory using chunking
            output_file_path = self.combine_csv_files(folder_path)

            # Step 6: Impute missing values in the 'name' feature
            processed_data = self.impute_missing_values(output_file_path)
            print("Data processing completed successfully.")

            return processed_data  # Use a different variable name to store the processed data
        except Exception as e:
            print(f"Error: {str(e)}")
            return None

    def get_csv_files(self):
        csv_files = []
        for file in os.listdir(self.data_directory):
            if file.endswith(".csv"):
                csv_files.append(os.path.join(self.data_directory, file))
        return csv_files

    def check_columns_in_csv_files(self, file_paths, columns_to_check):
        missing = []
        found = []
        for file_path in file_paths:
            print(f"Checking columns in file: {file_path}")
            try:
                df = pd.read_csv(file_path)
                missing_columns = set(columns_to_check) - set(df.columns)
                if missing_columns:
                    print(f"Missing columns in {file_path}: {', '.join(missing_columns)}")
                    missing.append({file_path: list(missing_columns)})
                else:
                    print(f"All required columns found in {file_path}.")
                    found.append(file_path)
            except Exception as e:
                print(f"Error while processing {file_path}: {e}")
        return missing, found

    def create_new_directory(self):
        new_directory = 'new_csv_files'
        os.makedirs(new_directory, exist_ok=True)
        return new_directory

    def modify_csv_file(self, csv_file, new_directory):
        chunksize = 100000
        reader = pd.read_csv(csv_file, chunksize=chunksize)
        df_copy = pd.concat([chunk.dropna(subset=['distinct_id']) for chunk in reader])
        new_csv_file = os.path.join(new_directory, f"new_{os.path.basename(csv_file)}")
        df_copy.to_csv(new_csv_file, index=False)
        print(f"Modified DataFrame saved to {new_csv_file}\n")

    def convert_distinct_id_to_numerical(self, folder_path):
        distinct_id_mapping = {}
        numerical_id_counter = 0
        csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]
        for csv_file in csv_files:
            csv_file_path = os.path.join(folder_path, csv_file)
            df = pd.read_csv(csv_file_path)
            for distinct_id in df['distinct_id'].unique():
                if distinct_id not in distinct_id_mapping:
                    distinct_id_mapping[distinct_id] = numerical_id_counter
                    numerical_id_counter += 1
            df['modified_distinct_id'] = df['distinct_id'].map(distinct_id_mapping)
            df.to_csv(csv_file_path, index=False)

    def combine_csv_files(self, folder_path):
        chunksize = 100000
        dfs = []
        features_to_keep = ['distinct_id', 'modified_distinct_id', 'time', 'name', 'currentScene', '$app_release',
                            '$ios_app_release', '$android_app_version', '$ios_app_version']
        csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]
        for csv_file in csv_files:
            file_path = os.path.join(folder_path, csv_file)
            with open(file_path, 'r') as f:
                first_line = f.readline().strip().split(',')
            if 'currentScene' in first_line and 'name' in first_line:
                reader = pd.read_csv(file_path, usecols=features_to_keep, chunksize=chunksize)
            else:
                reader = pd.read_csv(file_path, usecols=['distinct_id', 'modified_distinct_id', 'time', 'name', '$app_release', '$ios_app_release', '$android_app_version', '$ios_app_version'], chunksize=chunksize)
            df = pd.concat([chunk for chunk in reader])
            dfs.append(df)

        combined_df = pd.concat(dfs, ignore_index=True)
        output_directory = '/content/combined_csv_files'
        os.makedirs(output_directory, exist_ok=True)
        output_file = os.path.join(output_directory, 'output_file.csv')
        combined_df.to_csv(output_file, index=False)
        print("CSV files have been concatenated and saved to the new directory.")
        return output_file  # Return the path of the output file, not the DataFrame

    def impute_missing_values(self, output_file_path):  # Accept the path of the output file
        df8 = pd.read_csv(output_file_path)  # Read the DataFrame from the file path
        data_copy = df8.copy()
        mean_name = data_copy['name'].mean()
        data_copy['name'].fillna(mean_name, inplace=True)
        print("Missing values in 'name' feature imputed using mean.")
        return data_copy

    # Method to extract distinct ids
    def extract_distinct_ids(self):
        distinct_ids = set()
        try:
            distinct_ids.update(self.data['modified_distinct_id'].dropna().unique())
        except Exception as e:
            print(f"Error: An error occurred. {str(e)}")

        return distinct_ids

